{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adminn/Desktop/Repositories/database-chat-assistant'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "directory = os.getcwd()\n",
    "parent_directory = directory.split(\"/\")[:-3]\n",
    "parent_directory = \"/\".join(parent_directory)\n",
    "sys.path.append(parent_directory)\n",
    "parent_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.llms.groq import Groq\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "api_key=\"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\"\n",
    "llm = Groq(model=\"llama3-8b-8192\",\n",
    "    api_key=api_key,\n",
    "    temperature=0.5\n",
    ")\n",
    "response = llm.complete(\"hi\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bacis llama single document agent experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"LordOfTheRings.pdf\"]).load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embediing_model = HuggingFaceEmbedding(model_name='thenlper/gte-base')\n",
    "Settings.llm = llm\n",
    "Settings.embed_model=embediing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'creation_date': '2024-05-16',\n",
      "    'file_name': 'LordOfTheRings.pdf',\n",
      "    'file_path': 'LordOfTheRings.pdf',\n",
      "    'file_size': 13674248,\n",
      "    'file_type': 'application/pdf',\n",
      "    'last_modified_date': '2024-05-14',\n",
      "    'page_label': '13'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "pprint(nodes[12].metadata, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two indexes that llm will choose to use when quering the vDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex, StorageContext\n",
    "from app.utils.connections import chromadb_connection\n",
    "def store_in_chroma(nodes):\n",
    "    # creating or geting a connection from chromadb\n",
    "    vector_collection = chromadb_connection(\"vec_idx\")\n",
    "    # creating a vector store\n",
    "    chroma_vec_store = ChromaVectorStore(chroma_collection=vector_collection)\n",
    "    # Creating storage_context\n",
    "    vec_storage_context = StorageContext.from_defaults(vector_store=chroma_vec_store)\n",
    "    # creating index for summery and per page vector in db\n",
    "    vector_index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        show_progress=True,\n",
    "        storage_context=vec_storage_context\n",
    "    )\n",
    "    print(vector_collection.get(limit=5))\n",
    "    return vector_index\n",
    "\n",
    "index = store_in_chroma(nodes=nodes)\n",
    "# v, s = load_indices()\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing query engine to perform search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x7d7edd4a53c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# engine_summ = summary_index.as_query_engine(\n",
    "#     response_mode=\"tree_summarize\",\n",
    "#     use_async=True\n",
    "# )\n",
    "engine_vect = index.as_query_engine()\n",
    "\n",
    "# summary_tool = QueryEngineTool.from_defaults(\n",
    "#     query_engine=engine_summ,\n",
    "#     description=\"Used when queries are related to Summary of entire document.\"\n",
    "# )\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=engine_vect,\n",
    "    description=\"Used when queries require specific context instead of entire document.\"\n",
    ")\n",
    "engine_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Router using the query engine tools created using indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        # summary_tool,\n",
    "        vector_tool\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about the context of Durin's Bane, which is a specific concept in J.R.R. Tolkien's Middle-earth legendarium. The description 'Used when queries require specific context instead of entire document' suggests that this option is relevant to the question, as it implies that the context is important in understanding the concept..\n",
      "\u001b[0m\"Durin's Bane.\"\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = query_engine.query(\"what actually is durins bane\")\n",
    "pprint(str(response))\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funtion tool calling basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, show_progress=True)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "pprint(nodes[2].get_content(metadata_mode=\"all\"), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
