{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adminn/Desktop/Repositories/database-chat-assistant/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.llms.groq import Groq\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "api_key=\"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\"\n",
    "llm = Groq(model=\"llama3-8b-8192\",\n",
    "    api_key=api_key,\n",
    "    temperature=0.5\n",
    ")\n",
    "response = llm.complete(\"hi\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bacis llama single document agent experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"LordOfTheRings.pdf\"]).load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'creation_date': '2024-05-16',\n",
      "    'file_name': 'LordOfTheRings.pdf',\n",
      "    'file_path': 'LordOfTheRings.pdf',\n",
      "    'file_size': 13674248,\n",
      "    'file_type': 'application/pdf',\n",
      "    'last_modified_date': '2024-05-14',\n",
      "    'page_label': '13'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "pprint(nodes[12].metadata, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embediing_model = HuggingFaceEmbedding(model_name='thenlper/gte-base')\n",
    "Settings.llm = llm\n",
    "Settings.embed_model=embediing_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two indexes that llm will choose to use when quering the vDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1347/1347 [1:46:21<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from app.utils.connections import chromadb_connection\n",
    "summary_index = SummaryIndex(nodes, show_progress=True)\n",
    "summary_collection = chromadb_connection(\"sum_idx\")\n",
    "vector_index = VectorStoreIndex(nodes, show_progress=True)\n",
    "vector_collection = chromadb_connection(\"sum_idx\")\n",
    "chroma_vec_store = ChromaVectorStore(chroma_collection=vector_collection)\n",
    "chroma_doc_store = ChromaVectorStore(chroma_collection=summary_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing query engine to perform search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x7708056b7b80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "engine_summ = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True\n",
    ")\n",
    "engine_vect = vector_index.as_query_engine()\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=engine_summ,\n",
    "    description=\"Used when queries are related to Summary of entire document.\"\n",
    ")\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=engine_vect,\n",
    "    description=\"Used when queries require specific context instead of entire document.\"\n",
    ")\n",
    "engine_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Router using the query engine tools created using indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        # summary_tool,\n",
    "        vector_tool\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about the context of Durin's Bane, which is a specific concept in J.R.R. Tolkien's Middle-earth legendarium. The summary suggests that this concept requires specific context instead of the entire document, which is consistent with the idea that Durin's Bane is a specific entity that requires context to understand..\n",
      "\u001b[0mDurin's Bane is not explicitly mentioned in the provided context. However, it can be inferred that Durin's Bane is likely a powerful and ancient evil that has been awakened or is about to be awakened, given the descriptions of the world being fair in Durin's Day and the mention of mighty kings and cities that have passed away.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what actually is durins bane\")\n",
    "print(str(response))\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funtion tool calling basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, show_progress=True)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "pprint(nodes[2].get_content(metadata_mode=\"all\"), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
